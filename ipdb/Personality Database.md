The purpose of this project is that we want to collect information from the website personality-database.com, whereby we are going to try to hook into the different api endpoints located at https://api.personality-database.com/api/v1/, one such endpoint is https://api.personality-database.com/api/v1/profiles?offset=0&limit=100&cid=15&pid=1&cat_id=15&property_id=1 but there are several others being called by the website, this website is not complying with the GDPR and submits 500 requests per minute to data brokers and has useful socionics research, so I would like to dump the data from the api / database into a parquet file, so that we can use it to drive the socionics research comparisons to MBTI and big 5, by using the system prompt to prime the characters whose types are known, and then use them to answer a slate of survey questions, and then find the survey answers that have the higher inter-type K-L divergence to create an efficient test to correctly type individuals without just blatantly asking "are you an extrovert", but rather questions that the interviewee will not know how to decieve the interviewer. I would like to have all the data collected in a parquet file, and I would like to be able to search the parquet file databases with a vector search, with the vectors also stored in a parquet file, and joined on a primary key, where the primary key is the ipfs cid generated by the multiformats package ipfs_multiformats_py.