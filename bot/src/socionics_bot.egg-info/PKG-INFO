Metadata-Version: 2.4
Name: socionics-bot
Version: 0.1.0
Summary: Discord bot for Socionics research community with LLM-assisted responses
Author: Project Team
License: MIT
Requires-Python: >=3.11
Description-Content-Type: text/markdown
Requires-Dist: discord.py<3.0.0,>=2.3.2
Requires-Dist: httpx<0.28.0,>=0.27.0
Requires-Dist: pydantic<3.0.0,>=2.6.0
Requires-Dist: pydantic-settings<3.0.0,>=2.0.0
Requires-Dist: sentence-transformers<3.0.0,>=2.6.0
Requires-Dist: faiss-cpu>=1.7.4
Requires-Dist: pandas<3.0.0,>=2.2.0
Requires-Dist: pyarrow<20.0.0,>=16.0.0
Requires-Dist: orjson>=3.10.0
Requires-Dist: rich>=13.7.0
Requires-Dist: tenacity>=8.2.0
Requires-Dist: base58>=2.1.1
Requires-Dist: uvloop; platform_system == "Linux"
Requires-Dist: python-dotenv>=1.0.0
Requires-Dist: zstandard>=0.22.0
Requires-Dist: brotli>=1.1.0
Provides-Extra: dev
Requires-Dist: pytest; extra == "dev"
Requires-Dist: pytest-cov; extra == "dev"
Requires-Dist: ruff; extra == "dev"
Requires-Dist: mypy; extra == "dev"
Requires-Dist: types-requests; extra == "dev"

# Socionics Discord Bot (LLM-Assisted)

Purpose: Provide structured, safeguarded access to Socionics research information, educational explanations, and guided self-reflection prompts within the Discord community.

## Functional Scope (v0.1)
- /about_socionics: Neutral overview of theory + empirical status.
- /theory <topic>: Returns concise, sourced explanation (rate-limited).
- /intertype <type1> <type2>: Summarize canonical relation description + evidence gaps + suggested falsifiable questions.
- /reflect: Issues a randomized structured prompt (logged with prompt_id).
- /consent: Runs consent onboarding flow (integrates with repository governance service).
- /my_type_help: Provides a structured questionnaire to help users gather observations; DOES NOT assign a type automatically.
- /explain_functions: High-level definitions with caveats.
- /privacy: Displays data handling & logging details.
- /ingest_channel: Ingest recent messages (vectors + hashed metadata only) (admin).
- /search_vectors: Semantic similarity search (rate-limited).
- /keyword_search: Hybrid hashed token + semantic narrowing.
- /context_window: Builds context snippet metadata.
- /purge_message: Remove a specific message vector (admin).
- /llm_context: Returns JSON metadata for RAG assembly (no content).

## Out-of-Scope (Hard Guardrails)
- Direct assignment of a user's Socionics type.
- Personalized coaching or psychological advice.
- Medical or diagnostic claims.

## Conditional Guidance for Type Exploration
Workflow:
1. User runs /my_type_help.
2. Bot returns a 6-dimension self-observation checklist (energy focus, information seeking pattern, comfort/volition cues, decision framing, discourse style, feedback sensitivity).
3. User optionally answers follow-up questions (1–2 per dimension).
4. Bot summarizes patterns using neutral descriptors and suggests 2–3 candidate study tasks (e.g., record a monologue under two prompt categories) rather than naming a type.
5. If user persists in asking for a type, bot reiterates policy and offers resources: methodology doc link + explanatory article.

## Interpersonal Dynamics Explanation
- Provide standard relation category description (e.g., Duality) + highlight: "Empirical Evidence Status: unverified / limited / emerging".
- Encourage formulation of testable interactions: e.g., "Measure coordination time on novel tasks vs. matched non-Dual pairs." 

## Safeguards
- Profanity / harassment filter before LLM call.
- Red-team prompt tests at startup (ensure blocked outputs for disallowed requests).
- Response provenance: prepend banner if answer includes theoretical claims not yet empirically validated.
- Logging (hashed user ID, command, prompt_id, timestamp, model version, guardrail flags) to JSONL (rotated daily).
```
Discord Gateway → Command Router → Guardrail Pipeline → Intent Classifier → Tool/LLM Orchestrator → Response Formatter → Discord API
```

Guardrail Pipeline Components:

## Data Sources
- `docs/intro_socionics_research.md`
- `docs/operational_indicators.md`
- `docs/annotation_protocol.md`

## Future Extensions
- Semi-automated annotation suggestion for internal raters (/annotator mode).
- Multi-language support with translation quality confidence scores.
- Adaptive clarification questions (dialog state machine) for deeper concept explanations.

### Relationship Edges and IO Optimizations

- Edges: Related-profile relationships are recorded to `data/bot_store/pdb_profile_edges.parquet` via the `related` and `scan-all` flows. Each edge captures `(from_pid, to_pid, relation, source)` and is deduped; this avoids overwriting payloads just to record a new linkage.
- Upsert optimizations: `upsert_raw` and `upsert_vectors` skip parquet writes if the incoming payload/vector is byte-identical to what’s already stored for that CID. This reduces needless IO and log noise when the same items are seen from multiple seeds.

#### edges-report
Summarize the relationship graph stored in `pdb_profile_edges.parquet`.

Usage:
```
PYTHONPATH=bot/src python -m bot.pdb_cli edges-report --top 15
```
Output includes total edges, unique node count, and the top `N` PIDs by out-degree and in-degree.
Example output:
```
Edges: 721; Unique nodes: 259
Top out-degree:
	67657: 8
	...
Top in-degree:
	67206: 18
	...
```

#### edges-analyze
Analyze the relationship graph to compute connected components (undirected) and show the largest components with top-degree nodes.

Usage:
```
PYTHONPATH=bot/src python -m bot.pdb_cli edges-analyze --top 5 --per-component-top 10
```
Output includes total nodes/edges/components and, for each of the top `N` components, its size, number of edges, and nodes with highest degrees.

#### edges-export
Export per-node component membership and degree stats to a Parquet file for downstream analysis.

Usage:
```
PYTHONPATH=bot/src python -m bot.pdb_cli edges-export \
	--out data/bot_store/pdb_profile_edges_components.parquet
```
The output has columns: `pid`, `component`, `out_degree`, `in_degree`, `degree`.
It also prints a summary of the top components by node count.

## Local Development
Install & Run Tests:
```
python -m venv .venv
source .venv/bin/activate
pip install -e .[dev]
pytest -q
```

Run Bot (example):
```
export SOCIONICS_DISCORD_TOKEN=your_token_here
python -m bot.main
```

Salt Rotation (privacy maintenance):
```
export SOCIONICS_HASH_SALT=current_salt
python -m bot.maintenance NEW_LONG_RANDOM_SALT -y
```
This archives existing parquet hashed stores under data/bot_store/backup_<timestamp>/ and applies the new salt for subsequent ingests. Because original raw user IDs are never stored, historical hashed identifiers cannot be re-derived; rotation effectively resets the ingested vector corpus.

Structured JSON Logs:
```
export SOCIONICS_JSON_LOGS=true
python -m bot.main | jq '.'
```
Outputs fields: ts, level, logger, msg plus any extra contextual attributes.

## Admin & Maintenance
- Salt Rotation: `python -m bot.maintenance NEW_SALT -y` (archives old parquet stores under backup_<ts>/).
- Purge: `/purge_message <id>` removes vector + token hash entries.
- Admin Access: If SOCIONICS_ADMIN_ROLE_IDS set, role intersection required; else fallback to manage_messages permission.

## Privacy Summary
- Stored: embeddings, hashed user IDs (first 32 hex chars of salted SHA256), hashed tokens (first 16 hex chars), timestamps, channel/message IDs.
- Not Stored: raw message content, usernames, discriminators.
- Limitation: Salt rotation discards linkage to prior store (expected design for privacy); cannot migrate hashes.

## License
Inherits repository license.

## Data Ingestion CLI (PDB)

These commands support discovery-first ingestion from the Personality Database API with CID-keyed Parquet storage, embeddings, and FAISS search.

Global flags (override env):
- `--rpm`: Max requests per minute (overrides `PDB_RPM`)
- `--concurrency`: Parallel HTTP concurrency (overrides `PDB_CONCURRENCY`)
- `--timeout`: HTTP timeout in seconds (overrides `PDB_TIMEOUT_S`)
- `--base-url`: API base (overrides `PDB_API_BASE_URL`)
- `--headers`: Extra headers as JSON (merged last; overrides keys from `PDB_API_HEADERS`)

Example with globals:
```
PYTHONPATH=bot/src python -m bot.pdb_cli --rpm 120 --concurrency 8 --timeout 30 \
	search-top --pages 2 --limit 20 --query '' --auto-embed --auto-index
```
Example overriding base URL + headers:
```
PYTHONPATH=bot/src python -m bot.pdb_cli \
	--base-url https://api.personality-database.com/api/v2 \
	--headers '{"User-Agent":"Mozilla/5.0 ...","Referer":"https://www.personality-database.com/","Origin":"https://www.personality-database.com","Cookie":"X-Lang=en-US; ..."}' \
	search-top --pages 1 --limit 20 --query ''
```

### follow-hot
Resolves trending hot queries via v2 `search/top`. Upserts list-valued fields (e.g., `profiles`) and supports pagination.

Flags:
- `--limit`: Max results per page per query (default 10)
- `--max-keys`: Max number of hot query keys to follow (default 10)
- `--pages`: Number of pages to fetch via `nextCursor` for each key (default 1)
- `--until-empty`: Keep paging per key until an empty page
- `--next-cursor`: Starting `nextCursor` (default 0)
- `--auto-embed`: Run embedding after ingestion
- `--auto-index`: Rebuild FAISS index after ingestion (implies `--auto-embed`)
- `--index-out`: Index output path
- `--lists`: Comma-separated list names to upsert (e.g., `profiles,boards`)
- `--only-profiles`: Shortcut for `--lists profiles`
- `--dry-run`: Preview results without writing/upserting or embedding/indexing

Example:
```
PYTHONPATH=bot/src PDB_CACHE=1 PDB_API_BASE_URL=https://api.personality-database.com/api/v2 \
PDB_API_HEADERS='{"User-Agent":"Mozilla/5.0 ...","Referer":"https://www.personality-database.com/","Origin":"https://www.personality-database.com","Cookie":"X-Lang=en-US; ..."}' \
python -m bot.pdb_cli --rpm 90 --concurrency 6 --timeout 25 \
	follow-hot --max-keys 15 --limit 20 --pages 3 --auto-embed --auto-index --index-out data/bot_store/pdb_faiss.index
```

Dry-run (no writes):
```
PYTHONPATH=bot/src python -m bot.pdb_cli follow-hot --only-profiles --pages 2 --limit 20 --dry-run
```

### search-top
Queries v2 `search/top` directly, with paging and list filtering.

Flags:
- `--query` / `--keyword`: Query parameter (use `--encoded` if already URL-encoded)
- `--encoded`: Treat `--query` as already URL-encoded (e.g., `Elon%2520Musk`)
- `--limit`, `--next-cursor`, `--pages`, `--until-empty`: Pagination controls
- `--auto-embed`, `--auto-index`, `--index-out`: Post-ingest vector/index ops
- `--lists`, `--only-profiles`: Restrict which list arrays to upsert
- `--dry-run`: Preview results without writing/upserting or embedding/indexing

Examples:
```
PYTHONPATH=bot/src python -m bot.pdb_cli search-top --query '' --only-profiles --pages 1 --limit 20 --dry-run
PYTHONPATH=bot/src python -m bot.pdb_cli search-top --query 'Elon%2520Musk' --encoded --only-profiles --pages 1 --limit 20 --dry-run
```

### ingest-report
Summarizes items ingested via `search/top` and `follow-hot`, grouped by `_source_list` and top `_query` values.

Example:
```
PYTHONPATH=bot/src python -m bot.pdb_cli ingest-report --top-queries 10
```

### Ingestion Cycle Script
To run a full ingestion cycle end-to-end with environment-configurable headers, rate, and concurrency, use:

```
./scripts/pdb_ingest_cycle.sh
```

Environment overrides supported by the script:
- `PDB_BASE` (default `https://api.personality-database.com/api/v2`)
- `PDB_HEADERS` (JSON for headers incl. User-Agent, Referer, Origin, Cookie)
- `PDB_RPM`, `PDB_CONCURRENCY`, `PDB_TIMEOUT_S`
- `MAX_KEYS`, `PAGES`, `LIMIT`, `INDEX_OUT`
- `ONLY_PROFILES` (non-empty to pass `--only-profiles`)
- `LISTS` (e.g., `profiles,boards` to pass `--lists`)
- `DRY_RUN` (non-empty to pass `--dry-run`)
- `UNTIL_EMPTY` (non-empty to pass `--until-empty`)

### scan-related
Traverse seeds to collect v2 related profiles, optionally search by related names, and scrape v1 profiles for the discovered IDs. Supports dry-run and post-scrape embedding/indexing.

Flags:
- `--seed-ids`: Comma-separated seed profile IDs; if omitted, seeds are inferred from current raw parquet (up to `--max-seeds`).
- `--max-seeds`: Limit number of inferred seeds (default 100).
- `--depth`: Traversal depth (currently depth=1 supported).
- `--v1-base-url`, `--v1-headers`: Override base URL and headers for v1 profile fetches.
- `--search-names`: For each related item, call v2 `search/top` using its name.
- `--limit`, `--pages`, `--until-empty`: Pagination for name search.
- `--lists`, `--only-profiles`: Restrict list arrays to upsert from name search.
- `--auto-embed`, `--auto-index`, `--index-out`: Post-scrape vector/index ops.
- `--dry-run`: Preview without upserts/embedding/indexing.

Examples:
```
PYTHONPATH=bot/src python -m bot.pdb_cli scan-related --seed-ids 498239,12345 --search-names --only-profiles --pages 1 --limit 20 --dry-run

# Infer seeds from parquet, collect related, then scrape v1 profiles and index
PYTHONPATH=bot/src python -m bot.pdb_cli \
	--base-url https://api.personality-database.com/api/v2 \
	--headers '{"User-Agent":"Mozilla/5.0 ...","Referer":"https://www.personality-database.com/","Origin":"https://www.personality-database.com","Cookie":"X-Lang=en-US; ..."}' \
	scan-related --max-seeds 50 --search-names --only-profiles --pages 1 --limit 20 --auto-embed --auto-index
```

### scan-all
Iteratively expands coverage: pulls v2 related for a BFS-like traversal, optionally searches names via `search/top`, optionally sweeps generic tokens, and can scrape v1 profiles for discovered IDs. Supports persistent skip-state across runs and honors optional HTTP GET caching.

Key flags:
- `--max-iterations 0`: Run until exhaustion
- `--use-state` and `--state-file`: Persist and reuse progress between runs
- `--search-names`, `--limit`, `--pages`, `--until-empty`: Control name-search breadth
- `--sweep-queries a,b,c`, `--sweep-pages`, `--sweep-until-empty`, `--sweep-into-frontier`: Token sweeps to broaden discovery
- `--scrape-v1`, `--v1-base-url`, `--v1-headers`: Fetch v1 profiles for discovered IDs
- `--auto-embed`, `--auto-index`, `--index-out`: Maintain vectors and FAISS index

Example (stateful + cached):
```
export PDB_CACHE=1
PYTHONPATH=bot/src PDB_CACHE=1 python -m bot.pdb_cli \
	--rpm 90 --concurrency 3 --timeout 30 \
	--base-url https://api.personality-database.com/api/v2 \
	--headers "$(tr -d '\n' < .secrets/pdb_headers.json)" \
	scan-all --max-iterations 0 \
	--search-names --limit 20 --pages 1 --until-empty \
	--sweep-until-empty --sweep-into-frontier \
	--auto-embed --auto-index --index-out data/bot_store/pdb_faiss.index \
	--scrape-v1 --v1-base-url https://api.personality-database.com/api/v1 \
	--v1-headers "$(tr -d '\n' < .secrets/pdb_headers.json)" \
	--use-state
```
